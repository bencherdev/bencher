Das kontinuierliche Benchmarking ist eine Praxis in der Softwareentwicklung, bei der Mitglieder eines Teams ihre Arbeit regelm√§√üig bewerten.
In der Regel f√ºhrt jede Person mindestens t√§glich ein Benchmark durch - was zu mehreren Benchmarks pro Tag f√ºhrt.
Jedes Benchmark wird von einem automatisierten Build √ºberpr√ºft, um Leistungsregressionen so schnell wie m√∂glich zu erkennen.
Viele Teams finden, dass dieser Ansatz zu deutlich reduzierten Leistungseinbr√ºchen f√ºhrt
und es einem Team erm√∂glicht, performante Software schneller zu entwickeln.

Inzwischen ist jeder in der Softwarebranche mit Continuous Integration (CI) vertraut.
Auf einer grunds√§tzlichen Ebene geht es bei CI darum, Software-Feature-Regressions zu erkennen und zu verhindern, bevor sie in die Produktion gelangen.
√Ñhnlich verh√§lt es sich mit dem kontinuierlichen Benchmarking (CB): Es geht darum, Software-_Leistungsregressionen_ zu erkennen und zu verhindern, bevor sie in die Produktion gelangen.
Aus denselben Gr√ºnden, aus denen Unit-Tests in CI f√ºr jede Code√§nderung durchgef√ºhrt werden,
sollten Leistungstests in CB f√ºr jede Code√§nderung durchgef√ºhrt werden.
Diese Analogie ist so treffend, dass der erste Absatz dieses Abschnitts einfach eine Mad Libs-Version von [Martin Fowler's 2006 Einleitung zu Continuous Integration](https://martinfowler.com/articles/continuousIntegration.html) ist.

> üê∞ Performance-Bugs sind Fehler!

## Benchmarking in CI

Mythos: Benchmarking kann nicht in CI durchgef√ºhrt werden

Die meisten Benchmarking-Halterungen verwenden die [Systemwanduhr](https://en.wikipedia.org/wiki/Elapsed_real_time) zur Messung von Latenz oder Durchsatz.
Das ist sehr hilfreich, denn das sind genau die Metriken, um die wir uns als Entwickler am meisten k√ºmmern.
Allerdings sind allgemeine CI-Umgebungen oft laut und inkonsistent, wenn es darum geht, die Wanduhrzeit zu messen.
Bei kontinuierlichem Benchmarking f√ºgt diese Volatilit√§t unerw√ºnschtes Rauschen in die Ergebnisse ein.

Es gibt einige Optionen, um damit umzugehen:
- [Relatives Benchmarking](/de/docs/how-to/track-benchmarks/)
- Dedizierte CI-Runner
- Wechsel von Benchmark-Halterungen zu einer, die Anweisungen z√§hlt anstatt der Wandzeit

Oder einfach das Chaos annehmen! Kontinuierliches Benchmarking muss nicht perfekt sein.
Ja, die Reduzierung der Volatilit√§t und damit des Rauschens in Ihrer kontinuierlichen Benchmarking-Umgebung erm√∂glicht es Ihnen, immer feinere Leistungsregressionen zu erkennen.
Lassen Sie sich hier jedoch nicht von dem Gedanken leiten, dass Perfektion der Feind des Guten ist!

Sie sehen diese Grafik und denken vielleicht: "Wow, das ist verr√ºckt!" Aber fragen Sie sich: Kann Ihr aktueller Entwicklungsprozess eine Leistungsverschlechterung um das Zwei- oder sogar Zehnfache erkennen, bevor sie Ihre Nutzer beeinflusst? Wahrscheinlich nicht! _Das_ ist verr√ºckt!

Selbst bei all dem Rauschen aus einer CI-Umgebung kann das Nachverfolgen von Wanduhr-Benchmarks immer noch gro√üe Vorteile bringen, indem es Leistungsregressionen erkennt, bevor sie Ihre Kunden in der Produktion erreichen.
Mit der Zeit k√∂nnen Sie von dort aus weiter aufbauen, wenn Ihr Softwareleistungsmanagement reift.
In der Zwischenzeit verwenden Sie einfach Ihre regul√§re CI.

## Leistung z√§hlt!

Mythos: Man kann 100 ms Latenz nicht bemerken

Es ist √ºblich zu h√∂ren, dass Menschen behaupten, 100 ms Latenzzeit nicht wahrnehmen zu k√∂nnen.
Ein [Artikel der Nielsen Group √ºber Antwortzeiten](https://www.nngroup.com/articles/response-times-3-important-limits/) wird oft f√ºr diese Behauptung zitiert.

> **0,1 Sekunde** ist ungef√§hr die Grenze, bei der der Nutzer das Gef√ºhl hat, dass das System **sofort reagiert**, d. h. dass keine besondere R√ºckmeldung erforderlich ist, au√üer das Ergebnis anzuzeigen.
>
> - Jakob Nielsen, 1. Januar __*1993*__

Aber das stimmt einfach nicht.
Bei einigen Aufgaben k√∂nnen Menschen [gerade einmal 2 ms Latenzzeit wahrnehmen](https://pdfs.semanticscholar.org/386a/15fd85c162b8e4ebb6023acdce9df2bd43ee.pdf). Eine einfache M√∂glichkeit, dies zu beweisen, ist ein [Experiment von Dan Luu](https://danluu.com/input-lag/#appendix-why-measure-latency): √ñffnen Sie Ihre Konsole und f√ºhren Sie `sleep 0; echo "ping"` aus und dann `sleep 0,1; echo "pong"`. Sie haben den Unterschied bemerkt, oder‚ÄΩ

Ein weitere h√§ufige Verwechslung ist der Unterschied zwischen der Wahrnehmung von Latenz und menschlichen Reaktionszeiten. Obwohl es [etwa 200 ms dauert, auf einen visuellen Reiz zu reagieren](https://humanbenchmark.com/tests/reactiontime), ist das unabh√§ngig von der Wahrnehmung des Ereignisses selbst. Analog dazu k√∂nnen Sie bemerken, dass Ihr Zug zwei Minuten zu sp√§t ist (wahrgenommene Latenz), obwohl die Zugfahrt zwei Stunden dauert (Reaktionszeit).

Die Leistung z√§hlt! [Leistung ist ein Feature](https://blog.codinghorror.com/performance-is-a-feature)!

- Jede 100 ms schneller ‚Üí 1% mehr Konvertierungen ([Mobify](https://web.dev/why-speed-matters/), Einnahmen +380.000$/Jahr)
- 50% schneller ‚Üí 12% mehr Verk√§ufe ([AutoAnything](https://www.digitalcommerce360.com/2010/08/19/web-accelerator-revs-conversion-and-sales-autoanything/))
- 20% schneller ‚Üí 10% mehr Konvertierungen ([Furniture Village](https://www.thinkwithgoogle.com/intl/en-gb/marketing-strategies/app-and-mobile/furniture-village-and-greenlight-slash-page-load-times-boosting-user-experience/))
- 40% schneller ‚Üí 15% mehr Anmeldungen ([Pinterest](https://medium.com/pinterest-engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7))
- 850 ms schneller ‚Üí 7% mehr Konvertierungen ([COOK](https://web.dev/why-speed-matters/))
- Jede 1 sekunde langsamer ‚Üí 10% weniger Nutzer ([BBC](https://www.creativebloq.com/features/how-the-bbc-builds-websites-that-scale))

Mit dem Ende des Moore'schen Gesetzes m√ºssen Arbeitslasten, die parallel laufen k√∂nnen, parallelisiert werden.
Allerdings m√ºssen die meisten Arbeitslasten in Serie ausgef√ºhrt werden,
und einfach mehr Rechnerleistung auf das Problem zu werfen, wird schnell eine unl√∂sbare und teure L√∂sung.

Kontinuierliches Benchmarking ist ein Schl√ºsselelement zur Entwicklung und Pflege
leistungsf√§higer moderner Software angesichts dieser Ver√§nderung.

## Kontinuierliche Benchmarking-Tools

Bevor wir Bencher erstellt haben, haben wir nach einem Tool gesucht, das:

- Benchmarks in mehreren Sprachen verfolgen kann
- Sprachstandard-Benchmark-Ausgaben nahtlos integrieren kann
- Erweiterbar f√ºr benutzerdefinierten Benchmark-Ausgaben
- Open Source und f√§hig zur Selbst-Hosting
- Zusammenarbeit mit mehreren CI-Hosts
- Benutzerauthentifizierung und -autorisierung

Leider existierte nichts, das alle diese Kriterien erf√ºllte.
Siehe [bestehende L√∂sungen](/de/docs/reference/prior-art/) f√ºr eine umfassende Liste der bestehenden Benchmarking-Tools, von denen wir uns inspirieren lie√üen.

## Kontinuierliches Benchmarking in Big Tech

Tools wie Bencher wurden intern bei
Microsoft, Facebook (jetzt Meta), Apple, Amazon, Netflix und Google unter unz√§hligen anderen entwickelt.
Als Giganten der Branche verstehen sie die Bedeutung der Leistungsmessung w√§hrend der Entwicklung
und integrieren diese Erkenntnisse in den Entwicklungsprozess durch CB.
Wir haben Bencher entwickelt, um kontinuierliches Benchmarking aus dem Hinterzimmer der Big Tech in die Open-Source-Community zu bringen.
F√ºr Links zu Posts, die sich auf kontinuierliches Benchmarking in Big Tech beziehen, siehe [bestehende L√∂sungen](/de/docs/reference/prior-art/).
