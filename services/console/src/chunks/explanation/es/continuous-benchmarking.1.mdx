La Evaluaci√≥n Continua es una pr√°ctica de desarrollo de software en la cual los miembros de un equipo eval√∫an su trabajo frecuentemente,
generalmente cada persona realiza al menos una evaluaci√≥n diariamente, lo que resulta en m√∫ltiples evaluaciones por d√≠a.
Cada evaluaci√≥n es verificada por una construcci√≥n automatizada para detectar regresiones del desempe√±o tan r√°pidamente como sea posible.
Muchos equipos descubren que este enfoque conduce a una reducci√≥n significativa de las regresiones de desempe√±o
y permite a un equipo desarrollar software de alto rendimiento m√°s r√°pidamente.

Hasta ahora, todo el mundo en la industria de software conoce la integraci√≥n continua (CI).
A nivel fundamental, CI tiene que ver con la detecci√≥n y prevenci√≥n de regresiones de funciones de software antes de que lleguen a la producci√≥n.
De manera similar, el benchmarking continuo (CB) tiene como objetivo detectar y prevenir las regresiones en el _desempe√±o_ del software antes de que lleguen a la producci√≥n.
Por las mismas razones que las pruebas unitarias se ejecutan en CI para cada cambio de c√≥digo,
las pruebas de rendimiento deben ejecutarse en CB para cada cambio de c√≥digo.
Esta analog√≠a es tan precisa en realidad, que el primer p√°rrafo de esta secci√≥n es solo una versi√≥n Mad Libs de la [introducci√≥n de Martin Fowler a la Integraci√≥n Continua en 2006](https://martinfowler.com/articles/continuousIntegration.html).

> üê∞ ¬°Los errores de rendimiento son errores!

## Evaluaci√≥n en CI

Mito: No puedes ejecutar evaluaciones en CI

La mayor√≠a de los arneses de evaluaci√≥n utilizan el [reloj del sistema](https://en.wikipedia.org/wiki/Elapsed_real_time) para medir la latencia o el rendimiento.
Esto es muy √∫til, ya que estas son las m√©tricas exactas que a nosotros, como desarrolladores, m√°s nos importan.
Sin embargo, los entornos de CI de prop√≥sito general son a menudo ruidosos e inconsistentes al medir el tiempo de reloj.
Al realizar evaluaciones continuas, esta volatilidad agrega ruido no deseado a los resultados.

Existen algunas opciones para manejar esto:
- [Evaluaci√≥n relativa](/es/docs/how-to/track-benchmarks/)
- Corredores de CI dedicados
- Cambiar de arneses de evaluaci√≥n a uno que cuente las instrucciones en lugar del tiempo de reloj

¬°O simplemente abraza el caos! La evaluaci√≥n continua no tiene que ser perfecta.
S√≠, reducir la volatilidad y, por lo tanto, el ruido en tu entorno de evaluaci√≥n continua te permitir√° detectar regresiones de rendimiento cada vez m√°s finas.
Sin embargo, ¬°no dejes que lo perfecto sea enemigo de lo bueno aqu√≠!

<a href="https://bencher.dev/perf/bencher?key=true&measures=4358146b-b647-4869-9d24-bd22bb0c49b5&branches=619d15ed-0fbd-4ccb-86cb-fddf3124da29&tab=benchmarks&testbeds=0d991aac-b241-493a-8b0f-8d41419455d2&benchmarks=1db23e93-f909-40aa-bf42-838cc7ae05f5"><img src="https://api.bencher.dev/v0/projects/bencher/perf/img?branches=619d15ed-0fbd-4ccb-86cb-fddf3124da29&testbeds=0d991aac-b241-493a-8b0f-8d41419455d2&benchmarks=1db23e93-f909-40aa-bf42-838cc7ae05f5&measures=4358146b-b647-4869-9d24-bd22bb0c49b5&title=Embrace+the+Chaos%21" title="¬°Abraza el Caos!" alt="¬°Abraza el Caos! para Bencher - Bencher" /></a>

Puedes mirar este gr√°fico y pensar, "¬°Vaya, eso es una locura!" Pero preg√∫ntate a ti mismo, ¬øpuede tu proceso de desarrollo actual detectar un factor de dos o incluso un factor de diez de regresi√≥n de rendimiento antes de que afecte a tus usuarios? ¬°Probablemente no! ¬°Ahora _eso_ es una locura!

Incluso con todo el ruido de un entorno de CI, seguir las evaluaciones de tiempo de reloj puede seguir ofreciendo grandes dividendos en la detecci√≥n de regresiones de rendimiento antes de que lleguen a tus clientes en producci√≥n.
Con el tiempo, a medida que tu gesti√≥n de rendimiento de software madure, puedes partir de ah√≠.
Mientras tanto, simplemente usa tu CI regular.

## El Rendimiento Importa

Mito: No puedes notar 100ms de latencia

Es com√∫n escuchar a las personas afirmar que los humanos no pueden percibir 100ms de latencia.
Un [art√≠culo del Grupo Nielsen sobre tiempos de respuesta](https://www.nngroup.com/articles/response-times-3-important-limits/) es a menudo citado para esta afirmaci√≥n.

> **0.1 segundo** es aproximadamente el l√≠mite para que el usuario sienta que el sistema est√° **respondiendo instant√°neamente**, lo que significa que no es necesario ning√∫n feedback especial excepto para mostrar el resultado.
>
> - Jakob Nielsen, 1 Ene __*1993*__

Pero eso simplemente no es verdad.
En algunas tareas, las personas pueden percibir [hasta 2ms de latencia](https://pdfs.semanticscholar.org/386a/15fd85c162b8e4ebb6023acdce9df2bd43ee.pdf).
Una manera f√°cil de probar esto es un [experimento de Dan Luu](https://danluu.com/input-lag/#appendix-why-measure-latency): abre tu termial y ejecuta `sleep 0; echo "ping"` y luego ejecuta `sleep 0.1; echo "pong"`. ¬øNotaste la diferencia verdad‚ÄΩ

Otro punto com√∫n de confusi√≥n es la distinci√≥n entre la percepci√≥n de la latencia y los tiempos de reacci√≥n humanos. A pesar de que toma [alrededor de 200ms responder a un est√≠mulo visual](https://humanbenchmark.com/tests/reactiontime), eso es independiente de la percepci√≥n del evento en s√≠. Por analog√≠a, puedes darte cuenta de que tu tren llega dos minutos tarde (latencia percibida) aunque el viaje en tren tarde dos horas (tiempo de reacci√≥n).

¬°El rendimiento importa! ¬°[El rendimiento es una caracter√≠stica](https://blog.codinghorror.com/performance-is-a-feature/)!

- Cada 100ms m√°s r√°pido ‚Üí 1% m√°s de conversiones ([Mobify](https://web.dev/why-speed-matters/), ganando +$380,000/yr)
- 50% m√°s r√°pido ‚Üí 12% m√°s de ventas ([AutoAnything](https://www.digitalcommerce360.com/2010/08/19/web-accelerator-revs-conversion-and-sales-autoanything/))
- 20% m√°s r√°pido ‚Üí 10% m√°s de conversiones ([Furniture Village](https://www.thinkwithgoogle.com/intl/en-gb/marketing-strategies/app-and-mobile/furniture-village-and-greenlight-slash-page-load-times-boosting-user-experience/))
- 40% m√°s r√°pido ‚Üí 15% m√°s de inscripciones ([Pinterest](https://medium.com/pinterest-engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7))
- 850ms m√°s r√°pido ‚Üí 7% m√°s de conversiones ([COOK](https://web.dev/why-speed-matters/))
- Cada 1 segundo m√°s lento ‚Üí 10% menos usuarios ([BBC](https://www.creativebloq.com/features/how-the-bbc-builds-websites-that-scale))

Con la muerte de la Ley de Moore, las cargas de trabajo que pueden ejecutarse en paralelo necesitar√°n ser paralelizadas.
Sin embargo, la mayor√≠a de las cargas de trabajo necesitan ejecutarse en serie,
y simplemente lanzar m√°s c√°lculo al problema se est√° convirtiendo r√°pidamente en una soluci√≥n intratable y costosa.

La Evaluaci√≥n Continua es un componente clave para desarrollar y mantener
software de alto rendimiento moderno ante este cambio.

<div class="content has-text-centered">
<img
    src="https://s3.amazonaws.com/public.bencher.dev/moores_law.jpg"
    width="2124"
    height="1128"
    alt="La Ley de Moore desde https://davidwells.io/blog/rise-of-embarrassingly-parallel-serverless-compute"
/>
</div>

## Herramientas de Evaluaci√≥n Continua

Antes de crear Bencher, nos propusimos encontrar una herramienta que pudiera:

- Seguir evaluaciones a trav√©s de m√∫ltiples lenguajes
- Ingerir sin problemas la salida est√°ndar de los arneses de evaluaci√≥n de los lenguajes
- Extensible para salida personalizada de los arneses de evaluaci√≥n
- De c√≥digo abierto y capaz de auto-alojarse
- Trabajar con m√∫ltiples hosts de CI
- Autenticaci√≥n y autorizaci√≥n de usuarios

Desafortunadamente, no exist√≠a nada que cumpliera todos estos criterios.
Ver [arte previo](/es/docs/reference/prior-art/) para una lista completa de las herramientas de evaluaci√≥n existentes de las que nos inspiramos.

## Evaluaci√≥n Continua en las Grandes Empresas de Tecnolog√≠a

Herramientas como Bencher han sido desarrolladas internamente en
Microsoft, Facebook (ahora Meta), Apple, Amazon, Netflix, y Google entre innumerables otras.
Como los titanes de la industria, comprenden la importancia de monitorear el rendimiento durante el desarrollo
e integrar estos conocimientos en el proceso de desarrollo a trav√©s de la evaluaci√≥n continua.
Construimos Bencher para llevar la evaluaci√≥n continua desde detr√°s de las paredes de las grandes empresas de tecnolog√≠a a la comunidad de c√≥digo abierto.
Para enlaces a publicaciones relacionadas con la evaluaci√≥n continua de las grandes empresas de tecnolog√≠a, consulte [arte previo](/es/docs/reference/prior-art/).
